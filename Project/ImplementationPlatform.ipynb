{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TweetNLP Implementation\n",
        "This colab notebook is an adaptation of the demo notebook for TweetNLP, in which the developers provide a short introduction of [`tweetnlp`](https://github.com/cardiffnlp/tweetnlp), a python library of NLP models for tweets. \n",
        "\n",
        "For my project, I am going to be using the topic classification, offensive langugae, irony, and emotion classification models. So, I am going to focus on these areas as I demonstrate their function and test their accuracy.\n",
        "\n",
        "In order to test the accuracy of these various tasks, I am going to use the testing datasets from the parent paper and evaluate each task by F1 measure, as they did in the parent paper."
      ],
      "metadata": {
        "id": "lan4rltWDzvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Datasets and Packages"
      ],
      "metadata": {
        "id": "agIDKk3JeHL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#Emotion Classification Dataset\n",
        "e_url = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_text.txt\"\n",
        "emotions_txt = pd.read_csv(e_url, delimiter = \"\\n\", header = None)\n",
        "e_url2 = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_labels.txt\"\n",
        "emotions_true = pd.read_csv(e_url2, header = None)\n",
        "\n",
        "#Irony Detection Dataset\n",
        "i_url = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/test_text.txt\"\n",
        "irony_txt = pd.read_csv(i_url, delimiter = \"\\n\", header = None)\n",
        "i_url2 = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/test_labels.txt\"\n",
        "irony_true = pd.read_csv(i_url2, header = None)\n",
        "\n",
        "#Offensive Language Detection Dataset\n",
        "o_url = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt\"\n",
        "offensive_txt = pd.read_csv(o_url, delimiter = \"\\n\", header = None)\n",
        "o_url2 = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt\"\n",
        "offensive_true = pd.read_csv(o_url2, header = None)\n",
        "\n",
        "#Topic Classification Dataset\n",
        "#topic_url = \"\"\n",
        "#topic_txt = pd.read_csv(topic_url)\n",
        "#topic_url2 = \"\"\n",
        "#topic_true = pd.read_csv(topic_url2)"
      ],
      "metadata": {
        "id": "2aQPozJAaU2Z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "TweetNLP is available on pip or can be installed from source.\n"
      ],
      "metadata": {
        "id": "30SgbDvIEVC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Colab Error\n",
        "!pip install --upgrade google-cloud-storage"
      ],
      "metadata": {
        "id": "EgFq4wMPEpF6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffc2f493-6d0f-4db2-ea50-732081238909"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (1.18.1)\n",
            "Collecting google-cloud-storage\n",
            "  Downloading google_cloud_storage-2.6.0-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.31.6)\n",
            "Collecting google-resumable-media>=2.3.2\n",
            "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (1.35.0)\n",
            "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
            "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.56.4)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (2022.5)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.4)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.24.3)\n",
            "Installing collected packages: google-crc32c, google-resumable-media, google-cloud-core, google-cloud-storage\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-core-2.3.2 google-cloud-storage-2.6.0 google-crc32c-1.5.0 google-resumable-media-2.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7kTu1cyp-msS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6f546b-d919-4f54-d96b-1ed48b05a027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweetnlp\n",
            "  Downloading tweetnlp-0.1.2.tar.gz (25 kB)\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[K     |████████████████████████████████| 730 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tweetnlp) (1.21.6)\n",
            "Collecting urlextract\n",
            "  Downloading urlextract-1.7.1-py3-none-any.whl (20 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 59.4 MB/s \n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from tweetnlp) (1.12.1+cu113)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (3.7)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (0.4.2)\n",
            "Collecting traitlets>5.1.1\n",
            "  Downloading traitlets-5.5.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 69.7 MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (1.7.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 44.4 MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 56.6 MB/s \n",
            "\u001b[?25hCollecting lmdb>=1.2.1\n",
            "  Downloading lmdb-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting requests>=2.28\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (4.64.1)\n",
            "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (9.0.0)\n",
            "Collecting cached-path<1.2.0,>=1.1.3\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting termcolor==1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting fairscale==0.4.6\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 33.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting base58>=2.1.1\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (1.0.2)\n",
            "Collecting pytest>=6.2.5\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting filelock<3.8,>=3.3\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting sentencepiece>=0.1.96\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting h5py>=3.6.0\n",
            "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 36.5 MB/s \n",
            "\u001b[?25hCollecting spacy<3.4,>=2.1.0\n",
            "  Downloading spacy-3.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (0.3.6)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (3.17.3)\n",
            "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp->tweetnlp) (0.13.1+cu113)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.16\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 52.8 MB/s \n",
            "\u001b[?25hCollecting rich<13.0,>=12.1\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (2.6.0)\n",
            "Collecting boto3<2.0,>=1.0\n",
            "  Downloading boto3-1.26.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 49.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.4\n",
            "  Downloading botocore-1.29.4-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 47.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.4->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (1.35.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (2.3.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (1.31.6)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (2.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (1.56.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (2022.5)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (21.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (1.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp->tweetnlp) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp->tweetnlp) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp->tweetnlp) (4.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp->tweetnlp) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp->tweetnlp) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.5->allennlp->tweetnlp) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (0.4.8)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp->tweetnlp) (2.0.1)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.2.5->allennlp->tweetnlp) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.0.16->allennlp->tweetnlp) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp->tweetnlp) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp->tweetnlp) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->allennlp->tweetnlp) (2.10)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp->tweetnlp) (2.6.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.1->allennlp->tweetnlp) (3.1.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (0.7.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (3.0.10)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (0.6.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (1.0.3)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (0.10.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4,>=2.1.0->allennlp->tweetnlp) (1.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp->tweetnlp) (5.2.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp->tweetnlp) (7.1.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.8 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 74.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp->tweetnlp) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp->tweetnlp) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp->tweetnlp) (2.0.1)\n",
            "Collecting platformdirs\n",
            "  Downloading platformdirs-2.5.3-py3-none-any.whl (14 kB)\n",
            "Collecting uritools\n",
            "  Downloading uritools-4.0.0-py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: tweetnlp, fairscale, termcolor, jsonnet, pathtools, sacremoses, sentence-transformers\n",
            "  Building wheel for tweetnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweetnlp: filename=tweetnlp-0.1.2-py3-none-any.whl size=21393 sha256=465af7066170012173a7de17b6816c9d4498498ec43431a790b0c39833a5b0bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/94/62/516a4cba5a8daa9ca7c76f0251ea8b5a4adfd2cc32d4d0c7d8\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=fc1deea530057f4f5e88e5784925e9e27c741b2776cb3594df7df6f58914916a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=840d9944d0408e2340edbbecf2055dea358a5ce9982688efcccf51bb1234ea8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp37-cp37m-linux_x86_64.whl size=3997184 sha256=f97b97f44803dd8bb21699f27f4d630147bf729399b9f5f55f5e7d71d47b17ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/6b/48/a168ed5f8d01c50268605eff341c29126286763607bf707e3b\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=370d634e25254cfd8b15c738a68789a76ecb4c357e53b2ce7423f0d08912cadb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=56ab34cf091642960cc6ac3da584514469d5160cbd5174f73609aef720bece54\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=abfa5d6851d92ef99a058b709232f804158f40433ecca12ed825a3c141eec4d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built tweetnlp fairscale termcolor jsonnet pathtools sacremoses sentence-transformers\n",
            "Installing collected packages: urllib3, requests, jmespath, smmap, botocore, s3transfer, pydantic, gitdb, filelock, commonmark, tokenizers, thinc, shortuuid, setproctitle, sentry-sdk, rich, pluggy, pathtools, iniconfig, huggingface-hub, GitPython, exceptiongroup, docker-pycreds, boto3, wandb, uritools, transformers, traitlets, termcolor, tensorboardX, spacy, sentencepiece, sacremoses, pytest, platformdirs, lmdb, jsonnet, h5py, fairscale, cached-path, base58, urlextract, sentence-transformers, allennlp, tweetnlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.0.1\n",
            "    Uninstalling termcolor-2.0.1:\n",
            "      Successfully uninstalled termcolor-2.0.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.2\n",
            "    Uninstalling spacy-3.4.2:\n",
            "      Successfully uninstalled spacy-3.4.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 allennlp-2.10.1 base58-2.1.1 boto3-1.26.4 botocore-1.29.4 cached-path-1.1.6 commonmark-0.9.1 docker-pycreds-0.4.0 exceptiongroup-1.0.1 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.9 h5py-3.7.0 huggingface-hub-0.10.1 iniconfig-1.1.1 jmespath-1.0.1 jsonnet-0.19.1 lmdb-1.3.0 pathtools-0.1.2 platformdirs-2.5.3 pluggy-1.0.0 pydantic-1.8.2 pytest-7.2.0 requests-2.28.1 rich-12.6.0 s3transfer-0.6.0 sacremoses-0.0.53 sentence-transformers-2.2.2 sentencepiece-0.1.97 sentry-sdk-1.10.1 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 spacy-3.3.1 tensorboardX-2.5.1 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 traitlets-5.5.0 transformers-4.20.1 tweetnlp-0.1.2 uritools-4.0.0 urlextract-1.7.1 urllib3-1.26.12 wandb-0.12.21\n"
          ]
        }
      ],
      "source": [
        "# via pip\n",
        "!pip install tweetnlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip list | grep tweetnlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOEplcWa4Irs",
        "outputId": "69c0906b-5108-4f85-a86c-e18c9fe45d47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweetnlp                      0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All you need is to import `tweetnlp` !"
      ],
      "metadata": {
        "id": "RFS4uNJDFGOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweetnlp"
      ],
      "metadata": {
        "id": "E90e5oC6-4FQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tweet/Sentence Classification\n",
        "The classification module originally consisted of six different tasks (Sentiment Analysis, Irony Detection, Hate Detection, Offensive Detection, Emoji Prediction, and Emotion Analysis).\n",
        "\n",
        "In my project, I am going to be mainly using the emotion recognition and topic classification tasks to conduct my research. I am also going to attempt to utilize the hate, irony, and offensive language detection tasks and evaluate how this additional information is useful in my research. I will demonstrate the utility of each task and the accuracy of the results using a sample of the datasets used in the parent paper. \n",
        "\n",
        "In each example, the model is instantiated by `tweetnlp.load(\"task-name\")`, and run the prediction by giving a text or a list of \n",
        "texts."
      ],
      "metadata": {
        "id": "KAZYjeskBqL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic Classification\n",
        "The aim of this task is, given a tweet to assign topics related to its content. The task is formed as a supervised multi-label classification problem where each tweet is assigned one or more topics from a total of 19 available topics. The topics were carefully curated based on Twitter trends with the aim to be broad and general and consist of classes such as: arts and culture, music, or sports. Our internally-annotated dataset contains over 10K manually-labeled tweets."
      ],
      "metadata": {
        "id": "-997WsiT7B_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Topic Classification Demonstration"
      ],
      "metadata": {
        "id": "YxFQuXDrQeYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tweetnlp.load('topic_classification')  # Or `model = tweetnlp.TopicClassification()`\n",
        "model.topic(\"I went to Spain with my mom in December.\")  # Or `model.predict`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPGRUc-47EqM",
        "outputId": "7cb47ffd-ef2c-4e7b-df31-c9dc4caec94e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': ['diaries_&_daily_life', 'travel_&_adventure'],\n",
              " 'probability': {'arts_&_culture': 0.07455697655677795,\n",
              "  'business_&_entrepreneurs': 0.01791427470743656,\n",
              "  'celebrity_&_pop_culture': 0.024104885756969452,\n",
              "  'diaries_&_daily_life': 0.653227686882019,\n",
              "  'family': 0.1428426057100296,\n",
              "  'fashion_&_style': 0.018940361216664314,\n",
              "  'film_tv_&_video': 0.033042386174201965,\n",
              "  'fitness_&_health': 0.027939127758145332,\n",
              "  'food_&_dining': 0.0473339818418026,\n",
              "  'gaming': 0.01866302080452442,\n",
              "  'learning_&_educational': 0.032467249780893326,\n",
              "  'music': 0.029539255425333977,\n",
              "  'news_&_social_concern': 0.0279399361461401,\n",
              "  'other_hobbies': 0.02932649292051792,\n",
              "  'relationships': 0.0952962189912796,\n",
              "  'science_&_technology': 0.023787589743733406,\n",
              "  'sports': 0.020187407732009888,\n",
              "  'travel_&_adventure': 0.8862668871879578,\n",
              "  'youth_&_student_life': 0.029908694326877594}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Accuracy"
      ],
      "metadata": {
        "id": "fAAYLDPMR6Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on the testing data\n",
        "'''\n",
        "topic_pred = []\n",
        "tweet_list = topic_txt[topic_txt.columns[0]].values.tolist()\n",
        "label_list = [\"arts_&_culture\", \"business_&_entrepreneurs\", \"celebrity_&_pop_culture\", \"diaries_&_daily_life\", \"family\", \"fashion_&_style\", \"film_tv_&_video\", \"fitness_&_health\", \"food_&_dining\", \"gaming\", \"learning_&_educational\", \"music\", \"news_&_social_concern\", \"other_hobbies\", \"relationships\", \"science_&_technology\", \"sports\", \"travel_&_adventure\", \"youth_&_student_life\"]\n",
        "\n",
        "for tweet in tweet_list:\n",
        "  output = model.topic(tweet)\n",
        "  label = output[\"label\"]\n",
        "  label = label_list.index(label)\n",
        "  topic_pred.append(label)\n",
        "\n",
        "\n",
        "#Calculate F1 Measure\n",
        "topic_F1 = sklearn.metrics.f1_score(y_true=topic_true, y_pred=topic_pred, labels=None, pos_label=1, average='binary')\n",
        "'''\n",
        "\n",
        "print(\"Cannot test accuracy because of a lack of a labeled dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amo4B0C0_CyX",
        "outputId": "39f07f6e-68b4-4adb-cd3b-29fa9b325c61"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot test accuracy because of a lack of a labeled dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Irony Detection\n",
        "This is a binary classification task where given a tweet, the goal is to detect whether it is ironic or not. It is based on the Irony Detection dataset from the SemEval 2018 task."
      ],
      "metadata": {
        "id": "77r3HfbbCciI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Irony Detection Demonstration"
      ],
      "metadata": {
        "id": "GZ_hUqcbTktI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# single input\n",
        "model = tweetnlp.load('irony')  # Or `model = tweetnlp.Irony()` \n",
        "model.irony('Wow I love walking to class when its snowing')  # Or `model.predict`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKeJh8Bg_aOv",
        "outputId": "263ddaa4-965c-4b2e-841f-b9473eab5497"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'irony', 'probability': 0.9887392520904541}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Accuracy"
      ],
      "metadata": {
        "id": "2Z7UHdLyTnlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on the testing data\n",
        "irony_pred = []\n",
        "tweet_list = irony_txt[irony_txt.columns[0]].values.tolist()\n",
        "\n",
        "for tweet in tweet_list:\n",
        "  output = model.irony(tweet)\n",
        "  label = output[\"label\"]\n",
        "  if label == \"not-irony\":\n",
        "    label = 0\n",
        "  else:\n",
        "    label = 1\n",
        "  irony_pred.append(label)\n",
        "\n",
        "\n",
        "#Calculate F1 Measure\n",
        "irony_F1 = sklearn.metrics.f1_score(y_true=irony_true, y_pred=irony_pred, labels=None, pos_label=1, average='binary')"
      ],
      "metadata": {
        "id": "KbZzzO4wCeyc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Offensive Language Identification\n",
        "This task consists in identifying whether some form of offensive language is present in a tweet. For our benchmark we rely on the SemEval2019 OffensEval dataset."
      ],
      "metadata": {
        "id": "0zYehplyC_sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Offensive Language Identification Demonstration"
      ],
      "metadata": {
        "id": "kjb4SVHHTNmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# single input\n",
        "model = tweetnlp.load('offensive')  # Or `model = tweetnlp.Offensive()` \n",
        "model.offensive(\"Ohio State fans are losers\")  # Or `model.predict`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hty-v8KvAqDF",
        "outputId": "948f47dd-2889-43da-8155-b2384fe90437"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'offensive', 'probability': 0.8413265347480774}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Accuracy"
      ],
      "metadata": {
        "id": "98BgjojATRPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on the testing data\n",
        "offensive_pred = []\n",
        "tweet_list = offensive_txt[offensive_txt.columns[0]].values.tolist()\n",
        "\n",
        "for tweet in tweet_list:\n",
        "  output = model.offensive(tweet)\n",
        "  label = output[\"label\"]\n",
        "  if label == \"not-offensive\":\n",
        "    label = 0\n",
        "  else:\n",
        "    label = 1\n",
        "  offensive_pred.append(label)\n",
        "\n",
        "offensive_F1 = sklearn.metrics.f1_score(y_true=offensive_true, y_pred=offensive_pred, labels=None, pos_label=1, average='binary')"
      ],
      "metadata": {
        "id": "I0uTfYupDB__"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Emotion Recognition\n",
        "Given a tweet, this task consists of associating it with its most appropriate emotion. As a reference dataset we use the SemEval 2018 task on Affect in Tweets, simplified to only four emotions used in TweetEval: anger, joy, sadness and optimism."
      ],
      "metadata": {
        "id": "svmmP0WEDVV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Emotion Recognition Demonstration"
      ],
      "metadata": {
        "id": "q-6C9I-STG_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# single input\n",
        "model = tweetnlp.load('emotion')  # Or `model = tweetnlp.Emotion()` \n",
        "model.emotion('I cant wait to meet my new puppy')  # Or `model.predict`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TMXm5FcAtea",
        "outputId": "e6933c9a-9082-4c64-e2f3-55f707ae9af9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'joy', 'probability': 0.9166305661201477}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Accuracy"
      ],
      "metadata": {
        "id": "4ZtX52LcTJgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on the testing data\n",
        "emotions_pred = []\n",
        "tweet_list = emotions_txt[emotions_txt.columns[0]].values.tolist()\n",
        "\n",
        "for tweet in tweet_list:\n",
        "  output = model.emotion(tweet)\n",
        "  label = output[\"label\"]\n",
        "  if label == \"anger\":\n",
        "    label = 0\n",
        "  elif label == \"joy\":\n",
        "    label = 1\n",
        "  elif label == \"optimism\":\n",
        "    label = 2\n",
        "  else:\n",
        "    label = 3\n",
        "  emotions_pred.append(label)\n",
        "\n",
        "emotions_F1 = sklearn.metrics.f1_score(y_true=emotions_true, y_pred=emotions_pred, labels=None, pos_label=1, average=None)\n"
      ],
      "metadata": {
        "id": "33dz2H_3DYiJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Results"
      ],
      "metadata": {
        "id": "7G_9y_73thMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Emotions: {sum(emotions_F1)/4}, \\nIrony: {irony_F1}, \\nOffensive: {offensive_F1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZIA4HN2tk57",
        "outputId": "0479a2b2-486d-4413-d66c-814bae1acf4b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotions: 0.803827092044503, \n",
            "Irony: 0.5680365296803653, \n",
            "Offensive: 0.7239819004524886\n"
          ]
        }
      ]
    }
  ]
}